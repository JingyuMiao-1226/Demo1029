# app.py
# Streamlit ä¸­æ–‡æ–‡æœ¬å¸ƒå°”æ£€ç´¢ Dashboard
# è¿è¡Œï¼šstreamlit run app.py

import re
import requests
import pandas as pd
import streamlit as st
import altair as alt
from collections import defaultdict

st.set_page_config(page_title="æ£€ç´¢ Dashboard", layout="wide")

# st.title("ğŸ” ä¸­æ–‡æ–‡æœ¬æ£€ç´¢ Dashboard")
st.caption("ä» GitHub è¯»å–å¸¦è¯æ€§æ ‡æ³¨çš„æ–‡æœ¬ï¼Œé€šè¿‡å¸ƒå°”é€»è¾‘ (AND/OR/NOT) æ£€ç´¢å¥å­ã€‚")

# ------------------------
# åœ¨è¿™é‡Œå†…ç½®ä½ çš„ GitHub RAW æ–‡æœ¬åœ°å€
# ------------------------
GITHUB_FILES = {
    "è·¯é¥ã€Šå¹³å‡¡çš„ä¸–ç•Œã€‹": "https://raw.githubusercontent.com/JingyuMiao-1226/Demo1029/main/è·¯é¥ã€Šå¹³å‡¡çš„ä¸–ç•Œã€‹_pos.txt",
    "è€èˆã€Šéª†é©¼ç¥¥å­ã€‹": "https://raw.githubusercontent.com/JingyuMiao-1226/Demo1029/main/è€èˆã€Šéª†é©¼ç¥¥å­ã€‹_pos.txt",
    "ç‹å®‰å¿†ã€Šé•¿æ¨æ­Œã€‹": "https://raw.githubusercontent.com/JingyuMiao-1226/Demo1029/main/ç‹å®‰å¿†ã€Šé•¿æ¨æ­Œã€‹_pos.txt",
    "å¼ çˆ±ç²ã€ŠåŠç”Ÿç¼˜ã€‹": "https://raw.githubusercontent.com/JingyuMiao-1226/Demo1029/main/å¼ çˆ±ç²ã€ŠåŠç”Ÿç¼˜ã€‹_pos.txt",
}
# ---------------- å·¥å…·å‡½æ•° ----------------
@st.cache_data(show_spinner=False)
def fetch_text(url: str) -> str:
    try:
        r = requests.get(url, timeout=10)
        if r.status_code == 200:
            r.encoding = r.apparent_encoding or "utf-8"
            return r.text
    except Exception:
        pass
    return ""

def split_sentences(text: str):
    """ç®€å•ä¸­æ–‡åˆ†å¥"""
    text = re.sub(r"[ \t]+", " ", text.strip())
    return [s.strip() for s in re.split(r"[ã€‚ï¼ï¼Ÿ!?ï¼›;]\s*|\n+", text) if s.strip()]

def get_words(sentence: str):
    """æå–è¯ï¼ˆå¿½ç•¥è¯æ€§ï¼‰"""
    words = []
    for t in sentence.split():
        if "/" in t:
            w, _ = t.split("/", 1)
            words.append(w)
        else:
            words.append(t)
    return words

def eval_query(query: str, words: list):
    """
    ç®€åŒ–å¸ƒå°”é€»è¾‘ AND / OR / NOTï¼Œä»å·¦åˆ°å³é¡ºåºæ‰§è¡Œã€‚
    """
    q = query.upper().replace("(", " ( ").replace(")", " ) ")
    tokens = [t for t in q.split() if t]
    lw = [w.lower() for w in words]

    def term_match(term):
        return term.lower() in lw

    stack = []
    for t in tokens:
        if t == "NOT":
            if stack and isinstance(stack[-1], bool):
                stack[-1] = not stack[-1]
            else:
                stack.append(True)
        elif t == "AND":
            stack.append("AND")
        elif t == "OR":
            stack.append("OR")
        elif t in ("(", ")"):
            continue
        else:
            stack.append(term_match(t))

    result = None
    op = None
    for item in stack:
        if isinstance(item, bool):
            if result is None:
                result = item
            elif op == "AND":
                result = result and item
            elif op == "OR":
                result = result or item
        else:
            op = item
    return bool(result)

# ---------------- æ£€ç´¢åŒºï¼šåŒä¸€è¡Œå¸ƒå±€ ----------------
st.markdown("### ğŸ” æ£€ç´¢è®¾ç½®")
col1, col2 = st.columns([6, 1])
with col1:
    query = st.text_input("è¾“å…¥æ£€ç´¢è¯ï¼ˆæ”¯æŒ AND / OR / NOTï¼‰ï¼š", value="", label_visibility="collapsed")
with col2:
    search_btn = st.button("ğŸ” æ£€ç´¢")

# Enter æ£€ç´¢ï¼ˆå½“ query æ”¹å˜æ—¶ç«‹å³è§¦å‘ï¼‰æˆ–ç‚¹å‡»æŒ‰é’®éƒ½æ‰§è¡Œ
if search_btn or query:
    # ---------------- ä¸‹è½½æ–‡æœ¬ ----------------
    corpus, sentences_map = {}, {}
    for name, url in GITHUB_FILES.items():
        raw = fetch_text(url)
        corpus[name] = raw
        sentences_map[name] = split_sentences(raw) if raw else []

    if not any(sentences_map.values()):
        st.warning("æœªèƒ½ä»å†…ç½®çš„ GitHub RAW é“¾æ¥æ‹‰å–åˆ°æ–‡æœ¬ï¼Œè¯·æ›¿æ¢ä¸ºæœ‰æ•ˆçš„ RAW åœ°å€ã€‚")
        st.stop()

    # ---------------- æ£€ç´¢ ----------------
    rows = []
    match_counts = defaultdict(int)
    for fname, sents in sentences_map.items():
        for idx, s in enumerate(sents, start=1):
            words = get_words(s)
            if eval_query(query, words):
                rows.append({"æ–‡ä»¶": fname, "å¥å·": idx, "å¥å­ï¼ˆå«è¯æ€§ï¼‰": s})
                match_counts[fname] += 1

    # ---------------- Dashboard ----------------
    total = sum(match_counts.values())
    cols = st.columns(5)
    cols[0].metric("æ€»åŒ¹é…å¥å­æ•°", total)
    for i, name in enumerate(GITHUB_FILES.keys()):
        cols[i+1].metric(name, match_counts.get(name, 0))

    # ---------------- æŸ±çŠ¶å›¾ ----------------
    summary = pd.DataFrame({
        "æ–‡ä»¶": list(GITHUB_FILES.keys()),
        "åŒ¹é…å¥å­æ•°": [match_counts.get(n, 0) for n in GITHUB_FILES.keys()]
    })
    
    # ---------------- è¡¨æ ¼ç»“æœ ----------------
    st.markdown("### ğŸ“„ æ£€ç´¢ç»“æœï¼ˆå«è¯æ€§ï¼‰")
    if rows:
        st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
    else:
        st.info("æœªæ£€ç´¢åˆ°åŒ¹é…ç»“æœã€‚")
else:
    st.info("è¯·è¾“å…¥æ£€ç´¢è¯å¹¶ç‚¹å‡» **ğŸ” æ£€ç´¢** æˆ–æŒ‰ **Enter** å¼€å§‹ã€‚")
